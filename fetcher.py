# async_fetcher.py
import asyncio
import json
from datetime import datetime
import os
from playwright.async_api import async_playwright
from utils import get_config

max_posts = get_config("max_posts", 400)

async def extract_posts(page):
    posts = []
    items = await page.query_selector_all(".flow-item-row")

    for item in items:
        try:
            post_id = await (await item.query_selector(".box-id")).inner_text()
            timestamp = await (await item.query_selector(".box-header")).inner_text()
            content_element = await item.query_selector(".box-content span")
            content = await content_element.inner_text() if content_element else ""

            # æ—¶é—´æå–å’Œæ ¼å¼å¤„ç†
            try:
                date_str = timestamp.split()[-2]  # "05-28"
                time_str = timestamp.split()[-1]  # "12:05"
                date_object = datetime.strptime(date_str, "%m-%d")
                time_object = datetime.strptime(time_str, "%H:%M")
                date = date_object.strftime("%m-%d")
                time = time_object.strftime("%H:%M")
            except ValueError:
                date = "N/A"
                time = "N/A"

            posts.append({
                "id": post_id.strip(),
                "timestamp": timestamp.strip(),
                "content": content.strip(),
                "date": date,
                "time": time
            })
        except Exception as e:
            print(f"âŒ Error extracting post: {e}")
            continue

    return posts

def save_posts(posts, filename="treehole_posts.json"):
    if os.path.exists(filename):
        with open(filename, "r", encoding="utf-8") as f:
            try:
                existing_posts = json.load(f)
            except json.JSONDecodeError:
                existing_posts = []
    else:
        existing_posts = []

    new_posts = []
    existing_ids = {post["id"] for post in existing_posts}
    for post in posts:
        if post["id"] not in existing_ids:
            new_posts.append(post)

    all_posts = existing_posts + new_posts
    if len(all_posts) > max_posts:
        all_posts = all_posts[-max_posts:]

    with open(filename, "w", encoding="utf-8") as f:
        json.dump(all_posts, f, ensure_ascii=False, indent=2)

    return len(new_posts)

async def run_fetcher():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        
        # Check if auth.json exists
        if os.path.exists("auth.json"):
            context = await browser.new_context(storage_state="auth.json")
            print("ğŸŸ¢ å·²æ‰¾åˆ° auth.jsonï¼Œå°è¯•ä½¿ç”¨ç™»å½•çŠ¶æ€...")
        else:
            context = await browser.new_context()
            print("âš ï¸  æœªæ‰¾åˆ° auth.jsonï¼Œå°†æ‰“å¼€æµè§ˆå™¨è¿›è¡Œç™»å½•...")

        page = await context.new_page()

        await page.goto("https://treehole.pku.edu.cn/web/")

        # Check if already logged in
        if await page.locator('text=é€€å‡º').count() == 0:
            print("ğŸŸ¢ æ­£åœ¨æ‰“å¼€æ ‘æ´é¡µé¢ï¼Œè¯·ç™»å½•åæŒ‰å›è½¦ç»§ç»­...")
            input("ğŸ”‘ ç™»å½•å®ŒæˆåæŒ‰å›è½¦ä»¥ç»§ç»­æŠ“å–å¸–å­...")
            # Save storage state to auth.json
            await context.storage_state(path="auth.json")
            print("ğŸ’¾ ç™»å½•çŠ¶æ€å·²ä¿å­˜åˆ° auth.json")
        else:
            print("ğŸŸ¢ å·²ç™»å½•ï¼Œå¼€å§‹æŠ“å–å¸–å­...")

        while True:
            print(f"ğŸ” æ­£åœ¨åˆ·æ–°é¡µé¢å¹¶æŠ“å–æ•°æ® {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            await page.reload(wait_until="networkidle")
            posts = await extract_posts(page)
            saved_count = save_posts(posts)
            print(f"âœ… æˆåŠŸæŠ“å–å¹¶ä¿å­˜ {saved_count} æ¡å¸–å­")
            await asyncio.sleep(get_config("sleep_time", 60))

if __name__ == "__main__":
    asyncio.run(run_fetcher())
